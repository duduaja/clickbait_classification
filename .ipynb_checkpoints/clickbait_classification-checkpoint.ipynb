{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6981e924",
   "metadata": {},
   "source": [
    "# **import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb840fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b43a7448af40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba7fd11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: <built-in method __contains__ of dict object at 0x000001D2E7AD5CC0> returned a result with an error set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetTarget\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tf_session\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TF_SetConfig\n",
      "\u001b[1;31mImportError\u001b[0m: initialization failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#viz data\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional\n",
    "import sklearn.metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6962",
   "metadata": {},
   "source": [
    "# **load data**\n",
    "\n",
    "Data source: https://github.com/ruzcmc/ClickbaitIndo-textclassifier/blob/master/all_agree.csv <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2791a87a",
   "metadata": {},
   "source": [
    "**Clickbait Classification Indonesian Online News Headline** <br>\n",
    "The dataset consist of news headline that have been labeled clibckbait (1) or non-clickbait (0) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44eaaa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/ruzcmc/ClickbaitIndo-textclassifier/master/all_agree.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2521ce5",
   "metadata": {},
   "source": [
    "# **preprocessing part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb096f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comparing the number of clickbait and non-clickbait news headline\n",
    "print(raw_data['label'].value_counts())\n",
    "\n",
    "raw_data.groupby('label').size().plot(kind='pie',\n",
    "                                       y = 'label',\n",
    "                                       label = \"Type\",\n",
    "                                       autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bcaa6",
   "metadata": {},
   "source": [
    "from the pie chart we know that the data is unbalanced, with a ratio of 38.5% for clickbait and 61.5% for non-clickbait. so we have to balancing the data after cleaning it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e145aa",
   "metadata": {},
   "source": [
    "## text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowering text, and remove unnecessary stuff kinda punctuation, link, hashtag, and other\n",
    "#from 'Hai my name is @Annabelle' into 'hai my name is annabelle'\n",
    "def regex(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text= re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text= re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = re.sub(r'\\s*(https|http|com)',' ',text)\n",
    "    return text\n",
    "\n",
    "#tokenize sentence, from 'hai my name is annabelle', into ['hai','my','name','is','annabelle']\n",
    "def tokenize(x):\n",
    "    return word_tokenize(x)\n",
    "\n",
    "#remove unnecessary word in list, from ['hai','my','name','is','annabelle'], into ['name','annabelle']\n",
    "more_stopword = ['dengan', 'ia','bahwa','oleh','sebut','bikin','gara']\n",
    "list_stopwords = set(stopwords.words('Indonesian')+more_stopword)\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return [word for word in x if word not in list_stopwords]\n",
    "\n",
    "#unlist the list, from ['name','annabelle'], into 'name annabelle'\n",
    "def unlist(x):\n",
    "    return ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fb285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df,col_text,col_label):\n",
    "    df = df[[col_label,col_text]]\n",
    "    df.columns = ['y','text']\n",
    "    df['text'] = df['text'].apply(regex)\n",
    "    df['text'] = df['text'].apply(tokenize)\n",
    "    df['text'] = df['text'].apply(remove_stopwords)\n",
    "    df['text'] = df['text'].apply(unlist)\n",
    "    return df\n",
    "\n",
    "data_clean = preprocess(raw_data,'title','label_score')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec13b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_data = pd.DataFrame({'raw':raw_data['title'],\n",
    "                            'clean':data_clean['text']})\n",
    "compare_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f4b14",
   "metadata": {},
   "source": [
    "cleaning the data so the data has a same format (lowercase) and remove unnecessary elemen kinda punctuation, or non-relevant word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8235e",
   "metadata": {},
   "source": [
    "## balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3d054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb_text = data_clean[data_clean.y==1]\n",
    "ncb_text = data_clean[data_clean.y==0]\n",
    "\n",
    "#downsampling\n",
    "ncb_text = data_clean[data_clean.y==0].sample(n=len(cb_text),random_state=11)\n",
    "data_balance = cb_text.append(ncb_text).reset_index(drop=True)\n",
    "# balance_msg = spam_msg.append(ham_msg).reset_index(drop=True)\n",
    "# balance_msg\n",
    "data_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balance['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279347c",
   "metadata": {},
   "source": [
    "balancing is used for make a has a same number each number. now we have a same number of clickbait and non-clickbait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66be81",
   "metadata": {},
   "source": [
    "## wordcloud and most frequently words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcd037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_word_word:\n",
    "    def __init__(self,text_series,title=''):\n",
    "        self.title = title\n",
    "        self.text_series = text_series\n",
    "        self.title=title\n",
    "        \n",
    "        self.token_text = self.text_series.apply(tokenize)\n",
    "        self.token_word = []\n",
    "        for i in self.token_text:\n",
    "            for j in i:\n",
    "                self.token_word.append(j)\n",
    "\n",
    "        from nltk.probability import FreqDist\n",
    "        self.fdist = FreqDist(self.token_word)\n",
    "        self.fdist1 = dict(self.fdist.most_common(10))\n",
    "\n",
    "    def word_cloud(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from wordcloud import WordCloud\n",
    "        wordcloud = WordCloud(background_color=\"lavender\").generate_from_frequencies(self.fdist)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('wordcloud '+self.title, fontsize=20)#, fontweight='bold')\n",
    "        return plt.show()\n",
    "        \n",
    "    def most_common(self):\n",
    "        from nltk.probability import FreqDist\n",
    "        fdist1 = dict(self.fdist.most_common(10))\n",
    "        import matplotlib.pyplot as plt\n",
    "        key = list((self.fdist1).keys())\n",
    "        value = list((self.fdist1).values())\n",
    "        \n",
    "        def addlabels(x,y):\n",
    "            for i in range(len(x)):\n",
    "                plt.text(i, y[i], y[i], ha = 'center')\n",
    "        \n",
    "        plt.figure(figsize = (10, 5))\n",
    "        plt.bar(key, value)\n",
    "        addlabels(key, value)\n",
    "        plt.title(\"10 Most Frequently Word in {}\".format(self.title),fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d3611",
   "metadata": {},
   "source": [
    "### clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = word_word_word(cb_text['text'], 'Clickbait')\n",
    "cb.word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc079f",
   "metadata": {},
   "source": [
    "from the clickbait section, we can see that some of the words that appear most frequently are 'indonesia', 'foto','anak', 'habibie', 'viral', 'video', 'alasan', 'fakta', 'bj', 'kpk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833bdee",
   "metadata": {},
   "source": [
    "### non-clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncb = word_word_word(ncb_text['text'], 'non Clickbait')\n",
    "ncb.word_cloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78f0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncb.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fededb",
   "metadata": {},
   "source": [
    "from the non-clickbait section, we can see that some of the words that appear most frequently are 'kpk', 'indonesia', 'habibie', 'dpr', 'polisi', 'uu', 'jokowi', 'asap', 'rp', 'mahasiswa'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef5aff",
   "metadata": {},
   "source": [
    "# **preprocessing part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get length column for each text\n",
    "data_balance['text_length'] = data_balance['text'].apply(len)#Calculate average length by label types\n",
    "labels = data_balance.groupby('y').mean()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data_balance['text']\n",
    "y = data_balance['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3,\n",
    "                                                   random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c0faf",
   "metadata": {},
   "source": [
    "source code: https://towardsdatascience.com/nlp-spam-detection-in-sms-text-data-using-deep-learning-b8632db85cc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2070363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = 55\n",
    "trunc_type = \"post\" \n",
    "padding_type = \"post\" \n",
    "oov_tok = \"<OOV>\" \n",
    "vocab_size = 500\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, char_level=False, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word_index \n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd6a93",
   "metadata": {},
   "source": [
    "every words have a specific index or codes, so the sentence could be convert into word index. <br> \n",
    "example: <br>\n",
    "before: viral driver ojol bekasi pesanan makanan pakai sepeda <br>\n",
    "after : ----1-----2-----3-------4---------5-----------6---------7-------8--- <br>\n",
    "*ignore the dash (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9777b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing and padding on training and testing \n",
    "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences (training_sequences, maxlen = max_len, padding = padding_type, truncating = trunc_type )\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_len,\n",
    "padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ab674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train[0])\n",
    "print(training_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)\n",
    "\n",
    "# Before padding\n",
    "print(len(training_sequences[0]), len(training_sequences[1]))\n",
    "# After padding\n",
    "print(len(training_padded[0]), len(training_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f71d5",
   "metadata": {},
   "source": [
    "the data padded used to fulfill the input in modelling. This is relate with the mean of text length. We knew that the mean of text length is 49.99 for label '0' and 52.29 for label '1'. So if there any text with length less than that mean, gonna fill or padded with '0' value just for fulfill the model's input shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be441b",
   "metadata": {},
   "source": [
    "# **modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc439c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500 # As defined earlier\n",
    "embeding_dim = 16\n",
    "drop_value = 0.2 # dropout\n",
    "n_dense = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(drop_value))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam' ,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# fitting a dense spam detector model\n",
    "num_epochs = 40\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(training_padded, \n",
    "                    y_train, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, y_test),\n",
    "                    callbacks =[early_stop], \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance on test data \n",
    "model.evaluate(testing_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c45d4",
   "metadata": {},
   "source": [
    "**early stopping**\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_12.59.56_PM_1D7lrVF.png\" width=\"300\"><br>\n",
    "\n",
    "Early Stopping is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence, we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space.<br>\n",
    "\n",
    "source: <a href=\"https://paperswithcode.com/method/early-stopping\">Early Stopping Explained</a> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as a dataframe \n",
    "metrics = pd.DataFrame(history.history)\n",
    "# Rename column\n",
    "metrics.rename(columns = {'loss': 'Training_Loss', 'accuracy': 'Training_Accuracy', 'val_loss': 'Validation_Loss', 'val_accuracy': 'Validation_Accuracy'}, inplace = True)\n",
    "def plot_graphs1(var1, var2, string):\n",
    "    import matplotlib.pyplot as plt\n",
    "    metrics[[var1, var2]].plot()\n",
    "    plt.title('Training and Validation ' + string)\n",
    "    plt.xlabel ('Number of epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([var1, var2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b38b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Loss', 'Validation_Loss', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs1('Training_Accuracy', 'Validation_Accuracy', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model')\n",
    "\n",
    "new_model = tf.keras.models.load_model('my_model')\n",
    "new_model.predict(testing_padded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc712c58",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining prediction function\n",
    "def predict_spam(predict_msg):\n",
    "    new_seq = tokenizer.texts_to_sequences(predict_msg)\n",
    "    padded = pad_sequences(new_seq, maxlen =max_len,\n",
    "                      padding = padding_type,\n",
    "                      truncating=trunc_type)\n",
    "    return (model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into list\n",
    "list_y_test = list(y_test)\n",
    "list_x_test = list(x_test)\n",
    "\n",
    "#make the prediction and save it into list\n",
    "hasil_pred = []\n",
    "for i in list_x_test:\n",
    "    hasil = predict_spam([i])[0][0]\n",
    "    hasil_pred.append(round(hasil,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd55e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion matrix\n",
    "con_matrix = sklearn.metrics.confusion_matrix(list_y_test, hasil_pred)\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization confusion matrix\n",
    "cf_matrix = con_matrix\n",
    "\n",
    "group_names = ['TF','FP','FN','TN']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, \n",
    "            annot=labels, \n",
    "            fmt='', \n",
    "            cmap='Blues').set(title='confusion matrix',xlabel='actual', ylabel='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2612199",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (con_matrix[0][0] + con_matrix[-1][-1]) / np.sum(con_matrix)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50545751",
   "metadata": {},
   "source": [
    "## make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coba_text=['Viral Preman Palak Ibu Penjual Mie Pecal di Medan, Pelaku Ditangkap',\n",
    "           'Terkuak Alasan Jaksa Sebut Bharada Eliezer Bukan Justice Collaborator',\n",
    "           'Fakta-fakta Pembunuhan Berantai Wowon Cs di Bekasi-Cianjur',\n",
    "           'Menakar Seberapa Berpengaruh Ridwan Kamil Bagi Golkar di 2024',\n",
    "           'UU No. 6 Tahun 2014 soal Desa Penambahan Masa Jabatan Kepala Desa Hanya Melanggengkan Oligarki',\n",
    "           'Menag Usulkan Biaya Haji Naik Jadi Rp69 Juta untuk Jemaah']\n",
    "\n",
    "\n",
    "predict_spam(coba_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17933f9",
   "metadata": {},
   "source": [
    "The score of prediction obtained from classification model that represent clickbait probability. If the score gets closer to 1, then the news headline more clickbait."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
